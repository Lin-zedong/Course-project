# Course-project
Определение проблемы  
Студенты и преподаватели часто проверять сайт ruz.spbstu.ru вручную, чтобы узнать,
были ли внесены изменения в расписание; на платформе отсутствуют функции «добавить в избранное/подписаться» и «уведомление об изменениях»,
из-за чего легко пропустить обновления. Мы стремимся предоставить сервис,
позволяющий пользователям подписаться на расписание занятий/преподавателей/аудиторий,
которые они следят, и получать мгновенные уведомления через веб-сайт/Telegram/электронную почту при возникновении изменений.

Выработĸа требований  
1）Персоны  
   Студент (основной пользователь),  
   Преподаватель,  
   Администратор (аудит и метрики доставки)  

2）Пользовательские истории (текстом)  
   US-1 Подписка студента и быстрые оповещения  
   •	Как студент, когда в расписании моей группы меняются время/аудитория/преподаватель/отмена/добавление, я хочу получить сводное уведомление в течение 2 минут (Web/Telegram/Email) с различием «было → стало», чтобы оперативно скорректировать планы.  
   •	Критерии приёмки: создание подписки по groupId; появление записи в /events; дедупликация/дебаунс ≤120 c; уведомление содержит предмет, время, аудиторию, преподавателя и diff.  

   US-2 Отметка “важных” занятий  
   •	Как студент, когда просматриваю подписки, я хочу помечать отдельные занятия как важные, чтобы получать по ним приоритетные уведомления.  
   •	Критерии приёмки: important=true в подписке; важные изменения не агрегируются; маркировка [Важное] в уведомлении.  

   US-3 Мониторинг преподавателя и синхронизация со студентами  
   •	Как преподаватель, когда меняются мои занятия, я хочу сразу узнать и (опционально) разослать уведомление подписавшимся студентам, чтобы избежать недопонимания.  
   •	Критерии приёмки: подписка по teacherId; в событии — затронутые группы; в админке — статистика доставки.  

3）Оценка масштаба и сроков хранения  
   •	DAU(Ежедневная активность): ~10k. Пиковая одновременная нагрузка 300–500 пользователей.  
   •	Частота опроса: горячие субъекты каждые 30–60 с, холодные — 5–10 мин; ночная сверка.  
   •	Объём уведомлений: до 50k–100k/день в сезон пиков.  
   •	Хранение: события и аудит — ≥5 лет; сырые снапшоты — 30–90 дней; подписки/настройки — бессрочно до удаления.  

4）Нефункциональные цели  
   •	Концевая задержка детекции ≤ 2 мин;  
   •	P95 чтения (кеш) ≤ 300 мс;  
   •	Доступность 99.5%+/мес;  
   •	Аудит и переотправка для снижения ложных срабатываний.  

Разработĸа архитеĸтуры и детальное проеĸтирование  
1）Хараĸтер нагрузĸи на сервис  
   •	Соотношение R/W нагрузĸи:  
      R/W ≈ 20:1 (чтение: /events, /subscriptions; запись: создание/удаление подписок, фиксация событий и аудита).  
   •	Объёмы трафика (API и внешние вызовы)  
      Пользовательский API: 200–400 RPS в среднем, пик 1–2k RPS (при попаданиях в кеш).  
      Опрос внешнего источника RUZ:  
         «Горячие» субъекты (часто читаемые) — каждые 30–60 с  
         «Холодные» — каждые 5–10 мин  
         Ночная сверка — фоновая, с пониженными лимитами  
      Уведомления: 50k–100k/день в пиковые периоды (начало семестра/сессия), агрегируются дебаунсом.  
   •	Объёмы дисковой системы  
      “Горячие” данные (30 дней): Postgres 5–10 GB.  
      События + аудит: 0.5–1 GB/день (зависит от числа подписок и «шумности» RUZ).  
      Сырые снапшоты RUZ: храним 30–90 дней (старше — архив/удаление).  
      Долгосрочное хранение событий/аудита: ≥5 лет (требования курса).  
2)   
   •	L1 (System Context) — показаны акторы (Студент, Преподаватель, Администратор), наш сервис “Schedule Watcher” и внешние системы (RUZ, Telegram/Email) с направлениями взаимодействий (HTTP/JSON, Push/Email).  
   •	L2 (Container) — контейнеры внутри системы: Web/API, Core Service, Fetcher, Notifier, PostgreSQL, Redis, Message Queue, а также связи с RUZ и каналами уведомлений.  
   ![L1 (System Context)](C4modle/L1.drawio)  
   ![L2 (Container)](C4modle/L2.drawio)  
3) Контраĸты API + Ожидаемые нефунĸциональные требования на время отĸлиĸа  
   Базовые эндпойнты (v1):  
   •	GET /ping → {"status":"pong"} (P99 ≤ 50 мс)  
   •	POST /subscriptions — создать подписку:  
      Тело: { "type":"group|teacher|room", "id":"<ruz-id>", "channels":  
      ["web","telegram","email"], "filters":{...} } → 201 + {"subscriptionId":"..."}  
   •	GET /subscriptions?cursor&limit — пагинация списка  
   •	DELETE /subscriptions/{id} → 204 No Content  
   •	GET /events?since=<ts>&limit=<n> — события-изменения (курс, время/аудитория/преподаватель, diff «было→стало»)  
   •	GET /audit?subjectId=<id> (админ) — снапшоты, payload_hash, структурный diff, окно дебаунса, логи доставки  
   •	Ошибки: единый формат { traceId, code, message, details }  
   НФ-требования (SLO/SLI):  
   •	Чтение (кеш-хит) P95 ≤ 300 мс  
   •	E2E-детекция изменения (fetch → diff → debounce → send) ≤ 2 мин  
   •	Доступность ≥ 99.5% в месяц  
   •	Ограничение частоты на RUZ + экспоненциальные ретраи; идемпотентность операций записи  
4) Схему базы данных + почему она выдержит нефунĸциональные требования  
   Основные таблицы:  
   •	users(id UUID PK, email UNIQUE, telegram_id, created_at, status)  
   •	subjects(id PK, type ENUM, name, ruz_key UNIQUE)  
   •	subscriptions(id UUID PK, user_id FK, subject_id FK, channels JSONB, filters JSONB, created_at, UNIQUE(user_id,subject_id))  
   •	events(id BIGSERIAL PK, subject_id FK, event_time, diff JSONB, hash, created_at, UNIQUE(subject_id,hash))  
   •	snapshots(id BIGSERIAL PK, subject_id FK, snapshot_at, payload_hash, raw JSONB)  
   •	audit_logs(id BIGSERIAL PK, action, actor, target_id, meta JSONB, created_at)  
   Индексы и приёмы производительности:  
   •	Покрывающий индекс events(subject_id, event_time DESC) — быстрые «по времени» выборки.  
   •	Уникальность по user_id+subject_id и subject_id+hash — идемпотентность и защита от дубликатов.  
   •	Партиционирование events по времени (неделя/месяц) — дешёвые архив/очистка.  
   •	JSONB-пути на «горячих» полях diff (например, аудитория/время) — фильтрация без полного скана.  
   •	Разделение нагрузки: ведущая БД на запись + один/несколько read-реплик для /events и /subscriptions.  
   •	Redis — кеш «горячих» результатов и ограничение частоты вызовов.  
   Почему выполняются НФ-цели:  
   •	Быстрые чтения → покрывающие индексы + реплики + кеш.  
   •	Снижение записи → дедупликация событий по hash, дебаунс и пакетная обработка.  
   •	Предсказуемое время ответа → короткие транзакции, ограниченные JSONB-выборки, партиции.  
5) Схему масштабирования сервиса при росте нагрузĸи в 10 раз  
   •	Горизонтальное масштабирование Web/API, Fetcher, Notifier (K8s HPA по RPS/длине очереди/CPU).
   •	Чтение с реплик в Postgres; автомасштаб пулов соединений; разнос read/write.
   •	Очереди: партиционирование по subjectId, несколько потребителей в группе; DLQ для «тяжёлых» сообщений.
   •	Кеширование: Redis TTL 5–15 мин для горячих ключей; кеш-штамп и rate-limit на RUZ.
   •	Деградация: при недоступности RUZ — показ «последнего валидного снапшота» с явной меткой времени.
   •	Управление данными: автоархив партиций events, TTL для snapshots, регулярные VACUUM/REINDEX.
   •	Наблюдаемость: Prometheus/Grafana (RPS, P95, задержка fetch, глубина очереди, delivery-rate), трассировка OTel.
   •	Безопасные развёртывания: канареечные релизы/rollback, миграции схемы через версии.






